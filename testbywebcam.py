import sys
import cv2
import numpy as np
import tensorflow as tf
import mediapipe as mp
import random
from PyQt6.QtWidgets import QApplication, QWidget, QLabel, QVBoxLayout
from PyQt6.QtGui import QImage, QPixmap
from PyQt6.QtCore import QTimer, Qt
from collections import deque

# âœ… ëª¨ë¸ ë¡œë“œ (í•„ìš”ì‹œ compile=False ì˜µì…˜ ì¶”ê°€)
model = tf.keras.models.load_model("./Trained_model/best_transformer_model.keras", compile=False)

# âœ… ì œìŠ¤ì²˜ ë¦¬ìŠ¤íŠ¸
GESTURE_LIST = [
   'ê°€ì¡±,ì‹êµ¬,ì„¸ëŒ€,ê°€êµ¬',
    'ê°ê¸°',
    'ê±´ê°•,ê¸°ë ¥,ê°•ê±´í•˜ë‹¤,íŠ¼íŠ¼í•˜ë‹¤',
    'ê²€ì‚¬',
    'ê²°í˜¼,í˜¼ì¸,í™”í˜¼',
    'ê¿ˆ,í¬ë¶€,ê¿ˆê¾¸ë‹¤',
    'ë‚¨ë™ìƒ',
    'ë‚¨í¸,ë°°ìš°ì,ì„œë°©',
    'ë‚«ë‹¤,ì¹˜ìœ ',
    'ì‚´ë‹¤,ì‚¶,ìƒí™œ',
    'ì‰¬ë‹¤,íœ´ê°€,íœ´ê²Œ,íœ´ì‹,íœ´ì–‘',
    'ìŠµê´€,ë²„ë¦‡',
    'ì‹ ê¸°ë¡',
    'ì•ˆë…•,ì•ˆë¶€',
    'ì•½',
    'ì–‘ì¹˜ì§ˆ,ì–‘ì¹˜',
    'ì–´ë¨¸ë‹ˆ,ëª¨ì¹œ,ì–´ë¯¸,ì—„ë§ˆ',
    'ì—¬í–‰',
    'ì˜¤ë¹ ,ì˜¤ë¼ë²„ë‹ˆ',
    'ì·¨ë¯¸'
]

# âœ… MediaPipe Hands ì´ˆê¸°í™”
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

class SignLanguageApp(QWidget):
    def __init__(self):
        super().__init__()

        # âœ… UI ì„¤ì •
        self.setWindowTitle("âœ¨ Sign Language Quiz âœ¨")
        self.setStyleSheet("background-color: #eeeae1;")  
        self.showFullScreen()

        # âœ… ì›¹ìº  í™”ë©´ QLabel ì¶”ê°€
        self.video_label = QLabel(self)
        self.video_label.setAlignment(Qt.AlignmentFlag.AlignCenter)
        self.video_label.setFixedSize(640, 480)

        # âœ… ì˜ˆì¸¡ëœ ë‹¨ì–´ & ì •í™•ë„ í‘œì‹œ
        self.prediction_label = QLabel("ğŸ¯ Prediction: ?", self)
        self.prediction_label.setAlignment(Qt.AlignmentFlag.AlignLeft)
        self.prediction_label.setStyleSheet("font-size: 30px; color: #368f5f; font-weight: bold;")

        # âœ… ë ˆì´ì•„ì›ƒ ì„¤ì •
        main_layout = QVBoxLayout()
        main_layout.addWidget(self.prediction_label)
        main_layout.addWidget(self.video_label)
        self.setLayout(main_layout)

        # âœ… íƒ€ì´ë¨¸ ì„¤ì •
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_frame)
        self.sequence = []
        self.prediction_buffer = deque(maxlen=10)
        self.start_webcam()

    def start_webcam(self):
        self.cap = cv2.VideoCapture(0)
        if not self.cap.isOpened():
            print("âŒ ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        self.timer.start(30)

    def update_frame(self):
        ret, frame = self.cap.read()
        if not ret:
            return

        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        image = frame.copy()

        with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:
            results = hands.process(frame)

        # âœ… ì† ê°ì§€ í™•ì¸
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

        # âœ… OpenCV â†’ PyQt ë³€í™˜ í›„ QLabelì— í‘œì‹œ
        height, width, channel = frame.shape
        bytes_per_line = 3 * width
        q_img = QImage(frame.data, width, height, bytes_per_line, QImage.Format.Format_RGB888)
        self.video_label.setPixmap(QPixmap.fromImage(q_img))

        # âœ… ì†ì´ ê°ì§€ë˜ì—ˆì„ ë•Œë§Œ ë°ì´í„° ì €ì¥
        if results.multi_hand_landmarks:
            landmarks = np.zeros((30, 126), dtype=np.float32)
            for i, hand_landmarks in enumerate(results.multi_hand_landmarks[:2]):
                hand_data = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]).flatten()
                landmarks[:, i * 63:(i + 1) * 63] = hand_data

            self.sequence.append(landmarks)
            if len(self.sequence) > 30:
                self.sequence = self.sequence[-30:]

            # âœ… ì…ë ¥ ë°ì´í„° ì²´í¬ (ë””ë²„ê¹…ìš©)
            if len(self.sequence) == 30:
                input_data = np.array(self.sequence)
                print("ğŸ§ Input shape before expansion:", input_data.shape)

                if input_data.shape == (30, 30, 126):
                    input_data = input_data[:, 0, :]  # (30, 126)ë¡œ ë³€í™˜

                input_data = np.expand_dims(input_data, axis=0)
                print("âœ… Final input shape:", input_data.shape)

                # âœ… ì˜ˆì¸¡ ì‹¤í–‰
                predictions = model.predict(input_data)[0]
                self.prediction_buffer.append(predictions)

                # âœ… í”ë“¤ë¦¼ ë°©ì§€ (ìŠ¤ë¬´ë”© ì ìš©)
                smoothed_predictions = np.median(self.prediction_buffer, axis=0)
                max_index = np.argmax(smoothed_predictions)
                predicted_gesture = GESTURE_LIST[max_index]
                confidence = smoothed_predictions[max_index] * 100

                # âœ… ì˜ˆì¸¡ ê²°ê³¼ ì—…ë°ì´íŠ¸
                self.prediction_label.setText(f"ğŸ¯ Prediction: {predicted_gesture} ({confidence:.2f}%)")
                print(f"ğŸ” ì˜ˆì¸¡ ê²°ê³¼: {predicted_gesture} ({confidence:.2f}%)")

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    app = QApplication(sys.argv)
    window = SignLanguageApp()
    window.show()
    sys.exit(app.exec())
