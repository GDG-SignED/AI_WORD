import cv2
import numpy as np
import mediapipe as mp
from tensorflow.keras.models import load_model

# âœ… MediaPipe ì´ˆê¸°í™”
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

# âœ… AI ëª¨ë¸ ë¡œë“œ (ì†ë™ì‘ ì˜ˆì¸¡ Transformer ëª¨ë¸)
MODEL_PATH = "./Trained_model/trained_model/best_transformer_model.keras"
print("ğŸ”¹ ëª¨ë¸ ë¡œë“œ ì¤‘...")
model = load_model(MODEL_PATH)
print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

# âœ… ìˆ˜ì–´ ì œìŠ¤ì²˜ ë¦¬ìŠ¤íŠ¸ (ëª¨ë¸ì´ í•™ìŠµí•œ í´ë˜ìŠ¤ ìˆœì„œ)
GESTURE_LIST = [
    'ê°€ì¡±,ì‹êµ¬,ì„¸ëŒ€,ê°€êµ¬', 'ê°ê¸°', 'ê±´ê°•,ê¸°ë ¥,ê°•ê±´í•˜ë‹¤,íŠ¼íŠ¼í•˜ë‹¤', 'ê²€ì‚¬', 'ê²°í˜¼,í˜¼ì¸,í™”í˜¼', 
    'ê¿ˆ,í¬ë¶€,ê¿ˆê¾¸ë‹¤', 'ë‚¨ë™ìƒ', 'ë‚¨í¸,ë°°ìš°ì,ì„œë°©', 'ë‚«ë‹¤,ì¹˜ìœ ', 'ì‚´ë‹¤,ì‚¶,ìƒí™œ', 
    'ì‰¬ë‹¤,íœ´ê°€,íœ´ê²Œ,íœ´ì‹,íœ´ì–‘', 'ìŠµê´€,ë²„ë¦‡', 'ì‹ ê¸°ë¡', 'ì•ˆë…•,ì•ˆë¶€', 'ì•½', 
    'ì–‘ì¹˜ì§ˆ,ì–‘ì¹˜', 'ì–´ë¨¸ë‹ˆ,ëª¨ì¹œ,ì–´ë¯¸,ì—„ë§ˆ', 'ì—¬í–‰', 'ì˜¤ë¹ ,ì˜¤ë¼ë²„ë‹ˆ', 'ì·¨ë¯¸'
]

def extract_hand_landmarks(frame, hands):
    """ ë¹„ë””ì˜¤ í”„ë ˆì„ì—ì„œ ì† ëœë“œë§ˆí¬ (126ì°¨ì›) ì¶”ì¶œ """
    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(image)

    landmarks = np.zeros(126)  # ê¸°ë³¸ê°’ (ì†ì´ ê°ì§€ë˜ì§€ ì•Šìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€)
    if results.multi_hand_landmarks:
        temp_landmarks = []
        for hand_landmarks in results.multi_hand_landmarks:
            for lm in hand_landmarks.landmark:
                temp_landmarks.extend([lm.x, lm.y, lm.z])
        
        temp_landmarks = np.array(temp_landmarks[:126])  # ìµœëŒ€ 126ì°¨ì›ìœ¼ë¡œ ìë¥´ê¸°
        landmarks[:len(temp_landmarks)] = temp_landmarks  # ë¹ˆ ë¶€ë¶„ 0ìœ¼ë¡œ ì±„ìš°ê¸°

    return landmarks

def analyze_video(video_path):
    """ ë¹„ë””ì˜¤ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì†ë™ì‘ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜ """
    cap = cv2.VideoCapture(video_path)
    hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7, max_num_hands=2)

    sequence = []  # 30 í”„ë ˆì„ ë°ì´í„°ë¥¼ ì €ì¥í•  ë°°ì—´
    frame_count = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        landmarks = extract_hand_landmarks(frame, hands)  # ì† ëœë“œë§ˆí¬ ì¶”ì¶œ
        sequence.append(landmarks)
        frame_count += 1

        if frame_count == 30:  # 30 í”„ë ˆì„ì„ ìˆ˜ì§‘í•˜ë©´ ì¤‘ë‹¨
            break

    cap.release()
    hands.close()

    # í”„ë ˆì„ì´ ë¶€ì¡±í•˜ë©´ 0ìœ¼ë¡œ ì±„ìš°ê¸°
    while len(sequence) < 30:
        sequence.append(np.zeros(126))

    input_data = np.array(sequence, dtype=np.float32)
    input_data = np.expand_dims(input_data, axis=0)  # (1, 30, 126) í˜•íƒœë¡œ ë³€í™˜

    # âœ… ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰
    predictions = model.predict(input_data)
    predicted_index = np.argmax(predictions, axis=1)[0]
    predicted_gesture = GESTURE_LIST[predicted_index]
    confidence_score = predictions[0][predicted_index]

    return {
        "predicted_class": predicted_gesture,
        "confidence_score": float(confidence_score)
    }
