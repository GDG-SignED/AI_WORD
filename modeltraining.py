from matplotlib import pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import (
    Input, Dense, Dropout, LayerNormalization, MultiHeadAttention,
    GlobalAveragePooling1D
)
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
import os

# âœ… ìµœì  ëª¨ë¸ ì €ì¥ ê²½ë¡œ
model_save_path = "./Trained_model"
os.makedirs(model_save_path, exist_ok=True)

# âœ… ë°ì´í„° ë¡œë“œ
data_path = "./Processed_dataset"
X_train = np.load(os.path.join(data_path, "X_train.npy"))
y_train = np.load(os.path.join(data_path, "y_train.npy"))
X_val = np.load(os.path.join(data_path, "X_val.npy"))
y_val = np.load(os.path.join(data_path, "y_val.npy"))

# âœ… Transformer ëª¨ë¸ ì •ì˜ (ê³¼ì í•© ë°©ì§€ ì ìš©)
def build_transformer_model(input_shape, num_classes, d_model=128, num_heads=4, ff_dim=256):
    inputs = Input(shape=input_shape)

    # ğŸ”¥ Transformer ì¸ì½”ë” ë¸”ë¡ (L2 Regularization ì¶”ê°€)
    x = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(inputs, inputs)
    x = LayerNormalization(epsilon=1e-6)(x)
    x = Dropout(0.5)(x)  # ğŸ”¥ Dropout ì¦ê°€

    x = Dense(ff_dim, activation="relu", kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)
    x = Dropout(0.5)(x)  # ğŸ”¥ Dropout ì¦ê°€
    x = LayerNormalization(epsilon=1e-6)(x)

    x = GlobalAveragePooling1D()(x)
    x = Dense(128, activation="relu", kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)
    x = Dropout(0.5)(x)  # ğŸ”¥ Dropout ì¦ê°€
    outputs = Dense(num_classes, activation="softmax")(x)

    model = Model(inputs, outputs)
    return model

# âœ… ëª¨ë¸ ë¹Œë“œ
input_shape = (30, 126)
num_classes = y_train.shape[1]
model = build_transformer_model(input_shape, num_classes)

# âœ… ëª¨ë¸ ì»´íŒŒì¼
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.0005),
    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1),  # ğŸ”¥ Label Smoothing ì¶”ê°€
    metrics=["accuracy"]
)

# âœ… Early Stopping & Model Checkpoint ì„¤ì • (patience ì¡°ì •)
best_model_path = os.path.join(model_save_path, "./trained_model/best_transformer_model.keras")
callbacks = [
    EarlyStopping(monitor="val_loss", patience=8, restore_best_weights=True, verbose=1),
    ModelCheckpoint(best_model_path, monitor="val_loss", save_best_only=True, verbose=1),
    ReduceLROnPlateau(monitor="val_loss", factor=0.2, patience=5, verbose=1)  # ğŸ”¥ í•™ìŠµë¥  ê°ì†Œ ë¹„ìœ¨ ì¡°ì •
]

# âœ… ëª¨ë¸ í•™ìŠµ
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=200,
    batch_size=32,
    callbacks=callbacks
)

# âœ… Best ëª¨ë¸ ë¡œë“œ
model.load_weights(best_model_path)

# âœ… í•™ìŠµ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
def plot_training_history(history):
    plt.figure(figsize=(12, 5))

    # ğŸ”¹ Loss ê·¸ë˜í”„
    plt.subplot(1, 2, 1)
    plt.plot(history.history["loss"], label="Train Loss")
    plt.plot(history.history["val_loss"], label="Validation Loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend()
    plt.title("Training & Validation Loss")

    # ğŸ”¹ Accuracy ê·¸ë˜í”„
    plt.subplot(1, 2, 2)
    plt.plot(history.history["accuracy"], label="Train Accuracy")
    plt.plot(history.history["val_accuracy"], label="Validation Accuracy")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.title("Training & Validation Accuracy")

    plt.show()

# âœ… í•™ìŠµ ê·¸ë˜í”„ ì¶œë ¥
plot_training_history(history)

# âœ… ë°ì´í„° ë° ìƒ˜í”Œ ì¶œë ¥
print(f"X_train: {X_train.shape}, y_train: {y_train.shape}")
print(f"X_val: {X_val.shape}, y_val: {y_val.shape}")
print("ìƒ˜í”Œ ë°ì´í„°:\n", X_train[0])  # ì²« ë²ˆì§¸ ìƒ˜í”Œ ì¶œë ¥
print("ìƒ˜í”Œ ë¼ë²¨:\n", y_train[0])  # ì²« ë²ˆì§¸ ìƒ˜í”Œì˜ ë¼ë²¨ ì¶œë ¥
